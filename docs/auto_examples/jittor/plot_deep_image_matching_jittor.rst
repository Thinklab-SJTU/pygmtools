
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/jittor/plot_deep_image_matching_jittor.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_jittor_plot_deep_image_matching_jittor.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_jittor_plot_deep_image_matching_jittor.py:


==========================================================
Matching Image Keypoints by Graph Matching Neural Networks
==========================================================

This example shows how to match image keypoints by neural network-based graph matching solvers.
These graph matching solvers are designed to match two individual graphs. The matched images
can be further passed to tackle downstream tasks.

.. GENERATED FROM PYTHON SOURCE LINES 11-17

.. code-block:: default


    # Author: Runzhong Wang <runzhong.wang@sjtu.edu.cn>
    #         Wenzheng Pan <pwz1121@sjtu.edu.cn>
    #
    # License: Mulan PSL v2 License








.. GENERATED FROM PYTHON SOURCE LINES 19-28

.. note::
    The following solvers are based on matching two individual graphs, and are included in this example:

    * :func:`~pygmtools.neural_solvers.pca_gm` (neural network solver)

    * :func:`~pygmtools.neural_solvers.ipca_gm` (neural network solver)

    * :func:`~pygmtools.neural_solvers.cie` (neural network solver)


.. GENERATED FROM PYTHON SOURCE LINES 28-42

.. code-block:: default

    import jittor as jt # jittor backend
    from jittor import Var, models, nn
    import pygmtools as pygm
    import matplotlib.pyplot as plt # for plotting
    from matplotlib.patches import ConnectionPatch # for plotting matching result
    import scipy.io as sio # for loading .mat file
    import scipy.spatial as spa # for Delaunay triangulation
    from sklearn.decomposition import PCA as PCAdimReduc
    import itertools
    import numpy as np
    from PIL import Image
    pygm.BACKEND = 'jittor' # set default backend for pygmtools
    jt.flags.use_cuda = jt.has_cuda








.. GENERATED FROM PYTHON SOURCE LINES 43-55

Predicting Matching by Graph Matching Neural Networks
------------------------------------------------------
In this section we show how to do predictions (inference) by graph matching neural networks.
Let's take PCA-GM (:func:`~pygmtools.neural_solvers.pca_gm`) as an example.

Load the images
^^^^^^^^^^^^^^^^
Images are from the Willow Object Class dataset (this dataset also available with the Benchmark of ``pygmtools``,
see :class:`~pygmtools.dataset.WillowObject`).

The images are resized to 256x256.


.. GENERATED FROM PYTHON SOURCE LINES 55-69

.. code-block:: default

    obj_resize = (256, 256)
    img1 = Image.open('../data/willow_duck_0001.png')
    img2 = Image.open('../data/willow_duck_0002.png')
    kpts1 = jt.Var(sio.loadmat('../data/willow_duck_0001.mat')['pts_coord'])
    kpts2 = jt.Var(sio.loadmat('../data/willow_duck_0002.mat')['pts_coord'])
    kpts1[0] = kpts1[0] * obj_resize[0] / img1.size[0]
    kpts1[1] = kpts1[1] * obj_resize[1] / img1.size[1]
    kpts2[0] = kpts2[0] * obj_resize[0] / img2.size[0]
    kpts2[1] = kpts2[1] * obj_resize[1] / img2.size[1]
    img1 = img1.resize(obj_resize, resample=Image.BILINEAR)
    img2 = img2.resize(obj_resize, resample=Image.BILINEAR)
    jittor_img1 = jt.Var(np.array(img1, dtype=np.float32) / 256).permute(2, 0, 1).unsqueeze(0) # shape: BxCxHxW
    jittor_img2 = jt.Var(np.array(img2, dtype=np.float32) / 256).permute(2, 0, 1).unsqueeze(0) # shape: BxCxHxW





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /mnt/c/Users/liber/OneDrive/Documents/2022/pygmtools/examples/jittor/plot_deep_image_matching_jittor.py:64: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
      img1 = img1.resize(obj_resize, resample=Image.BILINEAR)
    /mnt/c/Users/liber/OneDrive/Documents/2022/pygmtools/examples/jittor/plot_deep_image_matching_jittor.py:65: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
      img2 = img2.resize(obj_resize, resample=Image.BILINEAR)




.. GENERATED FROM PYTHON SOURCE LINES 70-72

Visualize the images and keypoints


.. GENERATED FROM PYTHON SOURCE LINES 72-87

.. code-block:: default

    def plot_image_with_graph(img, kpt, A=None):
        plt.imshow(img)
        plt.scatter(kpt[0], kpt[1], c='w', edgecolors='k')
        if A is not None:
            for idx in jt.nonzero(A):
                plt.plot((kpt[0, idx[0]], kpt[0, idx[1]]), (kpt[1, idx[0]], kpt[1, idx[1]]), 'k-')

    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.title('Image 1')
    plot_image_with_graph(img1, kpts1)
    plt.subplot(1, 2, 2)
    plt.title('Image 2')
    plot_image_with_graph(img2, kpts2)




.. image-sg:: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_001.png
   :alt: Image 1, Image 2
   :srcset: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 88-93

Build the graphs
^^^^^^^^^^^^^^^^^
Graph structures are built based on the geometric structure of the keypoint set. In this example,
we refer to `Delaunay triangulation <https://en.wikipedia.org/wiki/Delaunay_triangulation>`_.


.. GENERATED FROM PYTHON SOURCE LINES 93-104

.. code-block:: default

    def delaunay_triangulation(kpt):
        d = spa.Delaunay(kpt.numpy().transpose())
        A = jt.zeros((len(kpt[0]), len(kpt[0])))
        for simplex in d.simplices:
            for pair in itertools.permutations(simplex, 2):
                A[pair] = 1
        return A

    A1 = delaunay_triangulation(kpts1)
    A2 = delaunay_triangulation(kpts2)








.. GENERATED FROM PYTHON SOURCE LINES 105-107

Visualize the graphs


.. GENERATED FROM PYTHON SOURCE LINES 107-115

.. code-block:: default

    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.title('Image 1 with Graphs')
    plot_image_with_graph(img1, kpts1, A1)
    plt.subplot(1, 2, 2)
    plt.title('Image 2 with Graphs')
    plot_image_with_graph(img2, kpts2, A2)




.. image-sg:: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_002.png
   :alt: Image 1 with Graphs, Image 2 with Graphs
   :srcset: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'jittor_core.Var' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'jittor_core.Var', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
      ary = asanyarray(ary)
    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
      ary = asanyarray(ary)




.. GENERATED FROM PYTHON SOURCE LINES 116-126

Extract node features via CNN
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Deep graph matching solvers can be fused with CNN feature extractors, to build an end-to-end learning pipeline.

In this example, let's adopt the deep graph solvers based on matching two individual graphs.
The image features are based on two intermediate layers from the VGG16 CNN model, following
existing deep graph matching papers (such as :func:`~pygmtools.neural_solvers.pca_gm`)

Let's firstly fetch and download the VGG16 model:


.. GENERATED FROM PYTHON SOURCE LINES 126-128

.. code-block:: default

    vgg16_cnn = models.vgg16_bn(True)








.. GENERATED FROM PYTHON SOURCE LINES 129-131

List of layers of VGG16:


.. GENERATED FROM PYTHON SOURCE LINES 131-133

.. code-block:: default

    print(vgg16_cnn.features)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Sequential(
        0: Conv(3, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[64,], None, Kw=None, fan=None, i=None, bound=None)
        1: BatchNorm(64, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        2: relu()
        3: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[64,], None, Kw=None, fan=None, i=None, bound=None)
        4: BatchNorm(64, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        5: relu()
        6: Pool((2, 2), (2, 2), padding=(0, 0), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=False, op=maximum)
        7: Conv(64, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[128,], None, Kw=None, fan=None, i=None, bound=None)
        8: BatchNorm(128, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        9: relu()
        10: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[128,], None, Kw=None, fan=None, i=None, bound=None)
        11: BatchNorm(128, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        12: relu()
        13: Pool((2, 2), (2, 2), padding=(0, 0), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=False, op=maximum)
        14: Conv(128, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[256,], None, Kw=None, fan=None, i=None, bound=None)
        15: BatchNorm(256, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        16: relu()
        17: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[256,], None, Kw=None, fan=None, i=None, bound=None)
        18: BatchNorm(256, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        19: relu()
        20: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[256,], None, Kw=None, fan=None, i=None, bound=None)
        21: BatchNorm(256, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        22: relu()
        23: Pool((2, 2), (2, 2), padding=(0, 0), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=False, op=maximum)
        24: Conv(256, 512, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[512,], None, Kw=None, fan=None, i=None, bound=None)
        25: BatchNorm(512, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        26: relu()
        27: Conv(512, 512, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[512,], None, Kw=None, fan=None, i=None, bound=None)
        28: BatchNorm(512, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        29: relu()
        30: Conv(512, 512, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[512,], None, Kw=None, fan=None, i=None, bound=None)
        31: BatchNorm(512, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        32: relu()
        33: Pool((2, 2), (2, 2), padding=(0, 0), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=False, op=maximum)
        34: Conv(512, 512, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[512,], None, Kw=None, fan=None, i=None, bound=None)
        35: BatchNorm(512, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        36: relu()
        37: Conv(512, 512, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[512,], None, Kw=None, fan=None, i=None, bound=None)
        38: BatchNorm(512, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        39: relu()
        40: Conv(512, 512, (3, 3), (1, 1), (1, 1), (1, 1), 1, float32[512,], None, Kw=None, fan=None, i=None, bound=None)
        41: BatchNorm(512, 1e-05, momentum=0.1, affine=True, is_train=True, sync=True)
        42: relu()
        43: Pool((2, 2), (2, 2), padding=(0, 0), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=False, op=maximum)
    )




.. GENERATED FROM PYTHON SOURCE LINES 134-137

Let's define the CNN feature extractor, which outputs the features of ``layer (30)`` and
``layer (37)``


.. GENERATED FROM PYTHON SOURCE LINES 137-149

.. code-block:: default

    class CNNNet(jt.nn.Module):
        def __init__(self, vgg16_module):
            super(CNNNet, self).__init__()
            # The naming of the layers follow ThinkMatch convention to load pretrained models.
            self.node_layers = jt.nn.Sequential(*[_ for _ in list(vgg16_module.features)[:31]])
            self.edge_layers = jt.nn.Sequential(*[_ for _ in list(vgg16_module.features)[31:38]])

        def execute(self, inp_img):
            feat_local = self.node_layers(inp_img)
            feat_global = self.edge_layers(feat_local)
            return feat_local, feat_global








.. GENERATED FROM PYTHON SOURCE LINES 150-153

Download pretrained CNN weights (from `ThinkMatch <https://github.com/Thinklab-SJTU/ThinkMatch>`_),
load the weights and then extract the CNN features


.. GENERATED FROM PYTHON SOURCE LINES 153-161

.. code-block:: default

    cnn = CNNNet(vgg16_cnn)
    path = pygm.utils.download('vgg16_pca_voc_jittor.pt', 'https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1qLxjcVq7X3brylxRJvELCbtCzfuXQ24J')
    cnn.load_state_dict(jt.load(path))

    with jt.no_grad():
        feat1_local, feat1_global = cnn(jittor_img1)
        feat2_local, feat2_global = cnn(jittor_img2)








.. GENERATED FROM PYTHON SOURCE LINES 162-164

Normalize the features


.. GENERATED FROM PYTHON SOURCE LINES 164-198

.. code-block:: default


    def local_response_norm(input: Var, size: int, alpha: float = 1e-4, beta: float = 0.75, k: float = 1.0) -> Var:
        """
        jittor implementation of local_response_norm
        """
        dim = input.ndim
        assert dim >= 3

        if input.numel() == 0:
            return input

        div = input.multiply(input).unsqueeze(1)
        if dim == 3:
            div = nn.pad(div, (0, 0, size // 2, (size - 1) // 2))
            div = nn.avg_pool2d(div, (size, 1), stride=1).squeeze(1)
        else:
            sizes = input.size()
            div = div.view(sizes[0], 1, sizes[1], sizes[2], -1)
            div = nn.pad(div, (0, 0, 0, 0, size // 2, (size - 1) // 2))
            div = nn.AvgPool3d((size, 1, 1), stride=1)(div).squeeze(1)
            div = div.view(sizes)
        div = div.multiply(alpha).add(k).pow(beta)
        return input / div


    def l2norm(node_feat):
        return local_response_norm(
            node_feat, node_feat.shape[1] * 2, alpha=node_feat.shape[1] * 2, beta=0.5, k=0)

    feat1_local = l2norm(feat1_local)
    feat1_global = l2norm(feat1_global)
    feat2_local = l2norm(feat2_local)
    feat2_global = l2norm(feat2_global)








.. GENERATED FROM PYTHON SOURCE LINES 199-201

Up-sample the features to the original image size and concatenate


.. GENERATED FROM PYTHON SOURCE LINES 201-209

.. code-block:: default

    feat1_local_upsample = jt.nn.interpolate(feat1_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat1_global_upsample = jt.nn.interpolate(feat1_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat2_local_upsample = jt.nn.interpolate(feat2_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat2_global_upsample = jt.nn.interpolate(feat2_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat1_upsample = jt.concat((feat1_local_upsample, feat1_global_upsample), dim=1)
    feat2_upsample = jt.concat((feat2_local_upsample, feat2_global_upsample), dim=1)
    num_features = feat1_upsample.shape[1]








.. GENERATED FROM PYTHON SOURCE LINES 210-212

Visualize the extracted CNN feature (dimensionality reduction via principle component analysis)


.. GENERATED FROM PYTHON SOURCE LINES 212-233

.. code-block:: default

    pca_dim_reduc = PCAdimReduc(n_components=3, whiten=True)
    feat_dim_reduc = pca_dim_reduc.fit_transform(
        np.concatenate((
            feat1_upsample.permute(0, 2, 3, 1).reshape(-1, num_features).numpy(),
            feat2_upsample.permute(0, 2, 3, 1).reshape(-1, num_features).numpy()
        ), axis=0)
    )
    feat_dim_reduc = feat_dim_reduc / np.max(np.abs(feat_dim_reduc), axis=0, keepdims=True) / 2 + 0.5
    feat1_dim_reduc = feat_dim_reduc[:obj_resize[0] * obj_resize[1], :]
    feat2_dim_reduc = feat_dim_reduc[obj_resize[0] * obj_resize[1]:, :]

    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.title('Image 1 with CNN features')
    plot_image_with_graph(img1, kpts1, A1)
    plt.imshow(feat1_dim_reduc.reshape(obj_resize[1], obj_resize[0], 3), alpha=0.5)
    plt.subplot(1, 2, 2)
    plt.title('Image 2 with CNN features')
    plot_image_with_graph(img2, kpts2, A2)
    plt.imshow(feat2_dim_reduc.reshape(obj_resize[1], obj_resize[0], 3), alpha=0.5)




.. image-sg:: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_003.png
   :alt: Image 1 with CNN features, Image 2 with CNN features
   :srcset: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'jittor_core.Var' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'jittor_core.Var', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
      ary = asanyarray(ary)
    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
      ary = asanyarray(ary)

    <matplotlib.image.AxesImage object at 0x7f20068d2550>



.. GENERATED FROM PYTHON SOURCE LINES 234-236

Extract node features by nearest interpolation


.. GENERATED FROM PYTHON SOURCE LINES 236-241

.. code-block:: default

    rounded_kpts1 = jt.round(kpts1).long()
    rounded_kpts2 = jt.round(kpts2).long()
    node1 = feat1_upsample[0, :, rounded_kpts1[1], rounded_kpts1[0]].t() # shape: NxC
    node2 = feat2_upsample[0, :, rounded_kpts2[1], rounded_kpts2[0]].t() # shape: NxC








.. GENERATED FROM PYTHON SOURCE LINES 242-246

Call PCA-GM matching model
^^^^^^^^^^^^^^^^^^^^^^^^^^
See :func:`~pygmtools.neural_solvers.pca_gm` for the API reference.


.. GENERATED FROM PYTHON SOURCE LINES 246-263

.. code-block:: default

    X = pygm.pca_gm(node1, node2, A1, A2, pretrain='voc')
    X = pygm.hungarian(X)

    plt.figure(figsize=(8, 4))
    plt.suptitle('Image Matching Result by PCA-GM')
    ax1 = plt.subplot(1, 2, 1)
    plot_image_with_graph(img1, kpts1, A1)
    ax2 = plt.subplot(1, 2, 2)
    plot_image_with_graph(img2, kpts2, A2)
    idx, _ = jt.argmax(X, dim=1)
    for i in range(X.shape[0]):
        j = idx[i].item()
        con = ConnectionPatch(xyA=kpts1[:, i], xyB=kpts2[:, j], coordsA="data", coordsB="data",
                              axesA=ax1, axesB=ax2, color="red" if i != j else "green")
        plt.gca().add_artist(con)





.. image-sg:: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_004.png
   :alt: Image Matching Result by PCA-GM
   :srcset: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'jittor_core.Var' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'jittor_core.Var', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
      ary = asanyarray(ary)
    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
      ary = asanyarray(ary)




.. GENERATED FROM PYTHON SOURCE LINES 264-273

Matching images with other neural networks
-------------------------------------------
The above pipeline also works for other deep graph matching networks. Here we give examples of
:func:`~pygmtoools.neural_solvers.ipca_gm` and :func:`~pygmtoools.neural_solvers.cie`.

Matching by IPCA-GM model
^^^^^^^^^^^^^^^^^^^^^^^^^
See :func:`~pygmtools.neural_solvers.ipca_gm` for the API reference.


.. GENERATED FROM PYTHON SOURCE LINES 273-279

.. code-block:: default

    path = pygm.utils.download('vgg16_ipca_voc_jittor.pt', 'https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1f7KEl9ZFZwI26j6UId-fsdl8Y8QWPKZi')
    cnn.load_state_dict(jt.load(path))

    feat1_local, feat1_global = cnn(jittor_img1)
    feat2_local, feat2_global = cnn(jittor_img2)








.. GENERATED FROM PYTHON SOURCE LINES 280-282

Normalize the features


.. GENERATED FROM PYTHON SOURCE LINES 282-291

.. code-block:: default

    def l2norm(node_feat):
        return local_response_norm(
            node_feat, node_feat.shape[1] * 2, alpha=node_feat.shape[1] * 2, beta=0.5, k=0)

    feat1_local = l2norm(feat1_local)
    feat1_global = l2norm(feat1_global)
    feat2_local = l2norm(feat2_local)
    feat2_global = l2norm(feat2_global)








.. GENERATED FROM PYTHON SOURCE LINES 292-294

Up-sample the features to the original image size and concatenate


.. GENERATED FROM PYTHON SOURCE LINES 294-302

.. code-block:: default

    feat1_local_upsample = jt.nn.interpolate(feat1_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat1_global_upsample = jt.nn.interpolate(feat1_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat2_local_upsample = jt.nn.interpolate(feat2_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat2_global_upsample = jt.nn.interpolate(feat2_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat1_upsample = jt.concat((feat1_local_upsample, feat1_global_upsample), dim=1)
    feat2_upsample = jt.concat((feat2_local_upsample, feat2_global_upsample), dim=1)
    num_features = feat1_upsample.shape[1]








.. GENERATED FROM PYTHON SOURCE LINES 303-305

Extract node features by nearest interpolation


.. GENERATED FROM PYTHON SOURCE LINES 305-310

.. code-block:: default

    rounded_kpts1 = jt.round(kpts1).long()
    rounded_kpts2 = jt.round(kpts2).long()
    node1 = feat1_upsample[0, :, rounded_kpts1[1], rounded_kpts1[0]].t() # shape: NxC
    node2 = feat2_upsample[0, :, rounded_kpts2[1], rounded_kpts2[0]].t() # shape: NxC








.. GENERATED FROM PYTHON SOURCE LINES 311-313

Build edge features as edge lengths


.. GENERATED FROM PYTHON SOURCE LINES 313-321

.. code-block:: default

    kpts1_dis = (kpts1.unsqueeze(0) - kpts1.unsqueeze(1))
    kpts1_dis = jt.norm(kpts1_dis, p=2, dim=2).detach()
    kpts2_dis = (kpts2.unsqueeze(0) - kpts2.unsqueeze(1))
    kpts2_dis = jt.norm(kpts2_dis, p=2, dim=2).detach()

    Q1 = jt.exp(-kpts1_dis / obj_resize[0])
    Q2 = jt.exp(-kpts2_dis / obj_resize[0])








.. GENERATED FROM PYTHON SOURCE LINES 322-324

Matching by IPCA-GM model


.. GENERATED FROM PYTHON SOURCE LINES 324-340

.. code-block:: default

    X = pygm.ipca_gm(node1, node2, A1, A2, pretrain='voc')
    X = pygm.hungarian(X)

    plt.figure(figsize=(8, 4))
    plt.suptitle('Image Matching Result by IPCA-GM')
    ax1 = plt.subplot(1, 2, 1)
    plot_image_with_graph(img1, kpts1, A1)
    ax2 = plt.subplot(1, 2, 2)
    plot_image_with_graph(img2, kpts2, A2)
    idx, _ = jt.argmax(X, dim=1)
    for i in range(X.shape[0]):
        j = idx[i].item()
        con = ConnectionPatch(xyA=kpts1[:, i], xyB=kpts2[:, j], coordsA="data", coordsB="data",
                              axesA=ax1, axesB=ax2, color="red" if i != j else "green")
        plt.gca().add_artist(con)




.. image-sg:: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_005.png
   :alt: Image Matching Result by IPCA-GM
   :srcset: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'jittor_core.Var' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'jittor_core.Var', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
      ary = asanyarray(ary)
    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
      ary = asanyarray(ary)




.. GENERATED FROM PYTHON SOURCE LINES 341-345

Matching by CIE model
^^^^^^^^^^^^^^^^^^^^^^
See :func:`~pygmtools.neural_solvers.cie` for the API reference.


.. GENERATED FROM PYTHON SOURCE LINES 345-351

.. code-block:: default

    path = pygm.utils.download('vgg16_cie_voc_jittor.pt', 'https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1wDbA-8sK4BNhA48z2c-Gtdd4AarRxfqT')
    cnn.load_state_dict(jt.load(path))

    feat1_local, feat1_global = cnn(jittor_img1)
    feat2_local, feat2_global = cnn(jittor_img2)








.. GENERATED FROM PYTHON SOURCE LINES 352-354

Normalize the features


.. GENERATED FROM PYTHON SOURCE LINES 354-363

.. code-block:: default

    def l2norm(node_feat):
        return local_response_norm(
            node_feat, node_feat.shape[1] * 2, alpha=node_feat.shape[1] * 2, beta=0.5, k=0)

    feat1_local = l2norm(feat1_local)
    feat1_global = l2norm(feat1_global)
    feat2_local = l2norm(feat2_local)
    feat2_global = l2norm(feat2_global)








.. GENERATED FROM PYTHON SOURCE LINES 364-366

Up-sample the features to the original image size and concatenate


.. GENERATED FROM PYTHON SOURCE LINES 366-374

.. code-block:: default

    feat1_local_upsample = jt.nn.interpolate(feat1_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat1_global_upsample = jt.nn.interpolate(feat1_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat2_local_upsample = jt.nn.interpolate(feat2_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat2_global_upsample = jt.nn.interpolate(feat2_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
    feat1_upsample = jt.concat((feat1_local_upsample, feat1_global_upsample), dim=1)
    feat2_upsample = jt.concat((feat2_local_upsample, feat2_global_upsample), dim=1)
    num_features = feat1_upsample.shape[1]








.. GENERATED FROM PYTHON SOURCE LINES 375-377

Extract node features by nearest interpolation


.. GENERATED FROM PYTHON SOURCE LINES 377-382

.. code-block:: default

    rounded_kpts1 = jt.round(kpts1).long()
    rounded_kpts2 = jt.round(kpts2).long()
    node1 = feat1_upsample[0, :, rounded_kpts1[1], rounded_kpts1[0]].t() # shape: NxC
    node2 = feat2_upsample[0, :, rounded_kpts2[1], rounded_kpts2[0]].t() # shape: NxC








.. GENERATED FROM PYTHON SOURCE LINES 383-385

Build edge features as edge lengths


.. GENERATED FROM PYTHON SOURCE LINES 385-393

.. code-block:: default

    kpts1_dis = (kpts1.unsqueeze(1) - kpts1.unsqueeze(2))
    kpts1_dis = jt.norm(kpts1_dis, p=2, dim=0).detach()
    kpts2_dis = (kpts2.unsqueeze(1) - kpts2.unsqueeze(2))
    kpts2_dis = jt.norm(kpts2_dis, p=2, dim=0).detach()

    Q1 = jt.exp(-kpts1_dis / obj_resize[0]).unsqueeze(-1).float32()
    Q2 = jt.exp(-kpts2_dis / obj_resize[0]).unsqueeze(-1).float32()








.. GENERATED FROM PYTHON SOURCE LINES 394-396

Call CIE matching model


.. GENERATED FROM PYTHON SOURCE LINES 396-412

.. code-block:: default

    X = pygm.cie(node1, node2, A1, A2, Q1, Q2, pretrain='voc')
    X = pygm.hungarian(X)

    plt.figure(figsize=(8, 4))
    plt.suptitle('Image Matching Result by CIE')
    ax1 = plt.subplot(1, 2, 1)
    plot_image_with_graph(img1, kpts1, A1)
    ax2 = plt.subplot(1, 2, 2)
    plot_image_with_graph(img2, kpts2, A2)
    idx, _ = jt.argmax(X, dim=1)
    for i in range(X.shape[0]):
        j = idx[i].item()
        con = ConnectionPatch(xyA=kpts1[:, i], xyB=kpts2[:, j], coordsA="data", coordsB="data",
                              axesA=ax1, axesB=ax2, color="red" if i != j else "green")
        plt.gca().add_artist(con)




.. image-sg:: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_006.png
   :alt: Image Matching Result by CIE
   :srcset: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_006.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'jittor_core.Var' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'jittor_core.Var', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
      ary = asanyarray(ary)
    /home/roger/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
      ary = asanyarray(ary)




.. GENERATED FROM PYTHON SOURCE LINES 413-429

Training a deep graph matching model
-------------------------------------
In this section, we show how to build a deep graph matching model which supports end-to-end training.
For the image matching problem considered here, the model is composed of a CNN feature extractor and
a learnable matching module. Take the PCA-GM model as an example.

.. note::
    This simple example is intended to show you how to do the basic execute and backward pass when
    training an end-to-end deep graph matching neural network. A 'more formal' deep learning pipeline
    should involve asynchronized data loader, batched operations, CUDA support and so on, which are
    all omitted in consideration of simplicity. You may refer to `ThinkMatch <https://github.com/Thinklab-SJTU/ThinkMatch>`_
    which is a research protocol with all these advanced features.

Let's firstly define the neural network model. By calling :func:`~pygmtools.utils.get_network`,
it will simply return the network object.


.. GENERATED FROM PYTHON SOURCE LINES 429-464

.. code-block:: default

    class GMNet(jt.nn.Module):
        def __init__(self):
            super(GMNet, self).__init__()
            self.gm_net = pygm.utils.get_network(pygm.pca_gm, pretrain=False) # fetch the network object
            self.cnn = CNNNet(vgg16_cnn)

        def execute(self, img1, img2, kpts1, kpts2, A1, A2):
            # CNN feature extractor layers
            feat1_local, feat1_global = self.cnn(img1)
            feat2_local, feat2_global = self.cnn(img2)
            feat1_local = l2norm(feat1_local)
            feat1_global = l2norm(feat1_global)
            feat2_local = l2norm(feat2_local)
            feat2_global = l2norm(feat2_global)

            # upsample feature map
            feat1_local_upsample = jt.nn.interpolate(feat1_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
            feat1_global_upsample = jt.nn.interpolate(feat1_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
            feat2_local_upsample = jt.nn.interpolate(feat2_local, (obj_resize[1], obj_resize[0]), mode='bilinear')
            feat2_global_upsample = jt.nn.interpolate(feat2_global, (obj_resize[1], obj_resize[0]), mode='bilinear')
            feat1_upsample = jt.concat((feat1_local_upsample, feat1_global_upsample), dim=1)
            feat2_upsample = jt.concat((feat2_local_upsample, feat2_global_upsample), dim=1)

            # assign node features
            rounded_kpts1 = jt.round(kpts1).long()
            rounded_kpts2 = jt.round(kpts2).long()
            node1 = feat1_upsample[0, :, rounded_kpts1[1], rounded_kpts1[0]].t()  # shape: NxC
            node2 = feat2_upsample[0, :, rounded_kpts2[1], rounded_kpts2[0]].t()  # shape: NxC

            # PCA-GM matching layers
            X = pygm.pca_gm(node1, node2, A1, A2, network=self.gm_net) # the network object is reused
            return X

    model = GMNet()








.. GENERATED FROM PYTHON SOURCE LINES 465-468

Define optimizer
^^^^^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 468-470

.. code-block:: default

    optim = jt.optim.Adam(model.parameters(), lr=1e-3)








.. GENERATED FROM PYTHON SOURCE LINES 471-474

Forward pass
^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 474-476

.. code-block:: default

    X = model(jittor_img1, jittor_img2, kpts1, kpts2, A1, A2)








.. GENERATED FROM PYTHON SOURCE LINES 477-482

Compute loss
^^^^^^^^^^^^^
In this example, the ground truth matching matrix is a diagonal matrix. We calculate the loss function via
:func:`~pygmtools.utils.permutation_loss`


.. GENERATED FROM PYTHON SOURCE LINES 482-486

.. code-block:: default

    X_gt = jt.init.eye(X.shape[0])
    loss = pygm.utils.permutation_loss(X, X_gt)
    print(f'loss={loss:.4f}')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    loss=2.9723




.. GENERATED FROM PYTHON SOURCE LINES 487-490

Backward Pass
^^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 490-492

.. code-block:: default

    optim.backward(loss)








.. GENERATED FROM PYTHON SOURCE LINES 493-495

Visualize the gradients


.. GENERATED FROM PYTHON SOURCE LINES 495-505

.. code-block:: default

    plt.figure(figsize=(4, 4))
    plt.title('Gradient Sizes of PCA-GM and VGG16 layers')
    plt.gca().set_xlabel('Layer Index')
    plt.gca().set_ylabel('Average Gradient Size')
    grad_size = []
    for param in model.parameters():
        grad_size.append(jt.abs(param.opt_grad(optim)).mean().item())
    print(grad_size)
    plt.stem(grad_size)




.. image-sg:: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_007.png
   :alt: Gradient Sizes of PCA-GM and VGG16 layers
   :srcset: /auto_examples/jittor/images/sphx_glr_plot_deep_image_matching_jittor_007.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [0.00011855587945319712, 0.0029571482446044683, 0.0001816817093640566, 0.003273540176451206, 0.0002074931690003723, 0.0049624452367424965, 1.0753510650829412e-05, 4.365541826700792e-05, 9.527620568405837e-05, 0.003951007500290871, 0.0001305502955801785, 0.002718730131164193, 0.0004479734634514898, 2.2476690730854898e-08, 0.0009480783483013511, 0.0006479034200310707, 0.0, 0.0, 0.0001895230234367773, 7.404293178581156e-09, 0.0019756690599024296, 0.0013428258243948221, 0.0, 0.0, 0.000257904757745564, 3.3749538719263228e-09, 0.001520381192676723, 0.0010707747424021363, 0.0, 0.0, 0.00022405723575502634, 3.886886812409784e-09, 0.002198364119976759, 0.0010921150678768754, 0.0, 0.0, 0.00021251743601169437, 1.351548650774248e-09, 0.001801412319764495, 0.0012037124251946807, 0.0, 0.0, 0.00017686065984889865, 2.415548427947556e-09, 0.0018887821352109313, 0.001236189273186028, 0.0, 0.0, 0.0001808749366318807, 2.7865243446001386e-09, 0.002013629302382469, 0.00098330934997648, 0.0, 0.0, 0.00015848233306314796, 6.819172448935262e-10, 0.001784672960639, 0.001079338020645082, 0.0, 0.0, 0.0001205688895424828, 1.459987464258461e-09, 0.0019706457387655973, 0.0011476018698886037, 0.0, 0.0, 0.00011520076805027202, 0.0005241598119027913, 0.001609515049494803, 0.000835501472465694, 0.0, 0.0, 9.032903471961617e-05, 4.2176875880706177e-10, 0.0016181940445676446, 0.0010808886727318168, 0.0, 0.0, 8.286116644740105e-05, 0.0008832790772430599]

    <StemContainer object of 3 artists>



.. GENERATED FROM PYTHON SOURCE LINES 506-509

Update the model parameters. A deep learning pipeline should iterate the forward pass
and backward pass steps until convergence.


.. GENERATED FROM PYTHON SOURCE LINES 509-512

.. code-block:: default

    optim.step()
    optim.zero_grad()








.. GENERATED FROM PYTHON SOURCE LINES 513-517

.. note::
    This example supports both GPU and CPU, and the online documentation is built by a CPU-only machine.
    The efficiency will be significantly improved if you run this code on GPU.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 10 minutes  34.226 seconds)


.. _sphx_glr_download_auto_examples_jittor_plot_deep_image_matching_jittor.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_deep_image_matching_jittor.py <plot_deep_image_matching_jittor.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_deep_image_matching_jittor.ipynb <plot_deep_image_matching_jittor.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
